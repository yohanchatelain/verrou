// Generated by './generateBackendInterOperator.py'
// generation of operation cast backend verrou


#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Int vr_verroucast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_verrou_cast_double_to_float(*arg1, &res, backend_verrou_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation cast backend mcaquad


#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(3) Int vr_mcaquadcast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_mcaquad_cast_double_to_float(*arg1, &res, backend_mcaquad_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation cast backend checkdenormal


#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(3) Int vr_checkdenormalcast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_checkdenormal_cast_double_to_float(*arg1, &res, backend_checkdenormal_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation cast backend vprec


#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(3) Int vr_vpreccast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_vprec_cast_double_to_float(*arg1, &res, backend_vprec_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation cast backend cancellation


#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(3) Int vr_cancellationcast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_cancellation_cast_double_to_float(*arg1, &res, backend_cancellation_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation cast backend bitmask


#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(3) Int vr_bitmaskcast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_bitmask_cast_double_to_float(*arg1, &res, backend_bitmask_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation cast backend mcaint


#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(3) Int vr_mcaintcast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_mcaint_cast_double_to_float(*arg1, &res, backend_mcaint_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation cast backend verrou


#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Int vr_verroucheckfloatmaxcast64FTo32F (Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_verrou_cast_double_to_float(*arg1, &res, backend_verrou_context);
  interflop_checkfloatmax_cast_double_to_float(*arg1, &res, backend_checkfloatmax_context);
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation add backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verrouadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrouadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrouadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrouadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}
#endif
// generation of operation sub backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verrousub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrousub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrousub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrousub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}
#endif
// generation of operation mul backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroumul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroumul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroumul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroumul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}
#endif
// generation of operation div backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroudiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroudiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroudiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroudiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i, backend_verrou_context);
  }
}
#endif
// generation of operation add backend mcaquad

#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(2) Long vr_mcaquadadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res + 1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}
#endif
// generation of operation sub backend mcaquad

#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(2) Long vr_mcaquadsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res + 1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}
#endif
// generation of operation mul backend mcaquad

#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(2) Long vr_mcaquadmul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_mul_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_mul_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_mul_double(arg1[1], arg2[1], res + 1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadmul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_mul_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_mul_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_mul_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}
#endif
// generation of operation div backend mcaquad

#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(2) Long vr_mcaquaddiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_div_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_div_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_div_double(arg1[1], arg2[1], res + 1, backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquaddiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_div_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_div_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_div_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
  }
}
#endif
// generation of operation add backend checkdenormal

#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(2) Long vr_checkdenormaladd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenormal_add_double(*arg1, *arg2, &res, backend_checkdenormal_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormaladd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenormal_add_double(arg1[0], arg2[0], res, backend_checkdenormal_context);
  interflop_checkdenormal_add_double(arg1[1], arg2[1], res + 1, backend_checkdenormal_context);
}

static VG_REGPARM(3) void vr_checkdenormaladd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormaladd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenormal_add_float(*arg1, *arg2, &res, backend_checkdenormal_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormaladd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenormal_add_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormaladd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_add_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}
#endif
// generation of operation sub backend checkdenormal

#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(2) Long vr_checkdenormalsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenormal_sub_double(*arg1, *arg2, &res, backend_checkdenormal_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenormal_sub_double(arg1[0], arg2[0], res, backend_checkdenormal_context);
  interflop_checkdenormal_sub_double(arg1[1], arg2[1], res + 1, backend_checkdenormal_context);
}

static VG_REGPARM(3) void vr_checkdenormalsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormalsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenormal_sub_float(*arg1, *arg2, &res, backend_checkdenormal_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenormal_sub_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormalsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_sub_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}
#endif
// generation of operation mul backend checkdenormal

#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(2) Long vr_checkdenormalmul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenormal_mul_double(*arg1, *arg2, &res, backend_checkdenormal_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalmul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenormal_mul_double(arg1[0], arg2[0], res, backend_checkdenormal_context);
  interflop_checkdenormal_mul_double(arg1[1], arg2[1], res + 1, backend_checkdenormal_context);
}

static VG_REGPARM(3) void vr_checkdenormalmul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormalmul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenormal_mul_float(*arg1, *arg2, &res, backend_checkdenormal_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalmul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenormal_mul_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormalmul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_mul_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}
#endif
// generation of operation div backend checkdenormal

#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(2) Long vr_checkdenormaldiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenormal_div_double(*arg1, *arg2, &res, backend_checkdenormal_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormaldiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenormal_div_double(arg1[0], arg2[0], res, backend_checkdenormal_context);
  interflop_checkdenormal_div_double(arg1[1], arg2[1], res + 1, backend_checkdenormal_context);
}

static VG_REGPARM(3) void vr_checkdenormaldiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormaldiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenormal_div_float(*arg1, *arg2, &res, backend_checkdenormal_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormaldiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenormal_div_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormaldiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_div_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
  }
}
#endif
// generation of operation add backend vprec

#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(2) Long vr_vprecadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_add_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_add_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_add_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_add_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}
#endif
// generation of operation sub backend vprec

#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(2) Long vr_vprecsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_sub_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_sub_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_sub_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_sub_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}
#endif
// generation of operation mul backend vprec

#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(2) Long vr_vprecmul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_mul_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecmul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_mul_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_mul_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecmul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecmul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_mul_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecmul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_mul_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecmul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_mul_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}
#endif
// generation of operation div backend vprec

#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(2) Long vr_vprecdiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_div_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecdiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_div_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_div_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecdiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecdiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_div_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecdiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_div_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecdiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_div_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}
#endif
// generation of operation add backend cancellation

#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(2) Long vr_cancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_cancellation_add_double(*arg1, *arg2, &res, backend_cancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_cancellation_add_double(arg1[0], arg2[0], res, backend_cancellation_context);
  interflop_cancellation_add_double(arg1[1], arg2[1], res + 1, backend_cancellation_context);
}

static VG_REGPARM(3) void vr_cancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_cancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(2) Int vr_cancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_cancellation_add_float(*arg1, *arg2, &res, backend_cancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_cancellation_add_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(3) void vr_cancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_cancellation_add_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}
#endif
// generation of operation sub backend cancellation

#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(2) Long vr_cancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_cancellation_sub_double(*arg1, *arg2, &res, backend_cancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_cancellation_sub_double(arg1[0], arg2[0], res, backend_cancellation_context);
  interflop_cancellation_sub_double(arg1[1], arg2[1], res + 1, backend_cancellation_context);
}

static VG_REGPARM(3) void vr_cancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_cancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(2) Int vr_cancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_cancellation_sub_float(*arg1, *arg2, &res, backend_cancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_cancellation_sub_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(3) void vr_cancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_cancellation_sub_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}
#endif
// generation of operation mul backend cancellation

#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(2) Long vr_cancellationmul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_cancellation_mul_double(*arg1, *arg2, &res, backend_cancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationmul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_cancellation_mul_double(arg1[0], arg2[0], res, backend_cancellation_context);
  interflop_cancellation_mul_double(arg1[1], arg2[1], res + 1, backend_cancellation_context);
}

static VG_REGPARM(3) void vr_cancellationmul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_cancellation_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(2) Int vr_cancellationmul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_cancellation_mul_float(*arg1, *arg2, &res, backend_cancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationmul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_cancellation_mul_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(3) void vr_cancellationmul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_cancellation_mul_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}
#endif
// generation of operation div backend cancellation

#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(2) Long vr_cancellationdiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_cancellation_div_double(*arg1, *arg2, &res, backend_cancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationdiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_cancellation_div_double(arg1[0], arg2[0], res, backend_cancellation_context);
  interflop_cancellation_div_double(arg1[1], arg2[1], res + 1, backend_cancellation_context);
}

static VG_REGPARM(3) void vr_cancellationdiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_cancellation_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(2) Int vr_cancellationdiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_cancellation_div_float(*arg1, *arg2, &res, backend_cancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationdiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_cancellation_div_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}

static VG_REGPARM(3) void vr_cancellationdiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_cancellation_div_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
  }
}
#endif
// generation of operation add backend bitmask

#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(2) Long vr_bitmaskadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_bitmask_add_double(*arg1, *arg2, &res, backend_bitmask_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_bitmask_add_double(arg1[0], arg2[0], res, backend_bitmask_context);
  interflop_bitmask_add_double(arg1[1], arg2[1], res + 1, backend_bitmask_context);
}

static VG_REGPARM(3) void vr_bitmaskadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_bitmask_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(2) Int vr_bitmaskadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_bitmask_add_float(*arg1, *arg2, &res, backend_bitmask_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_bitmask_add_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(3) void vr_bitmaskadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_bitmask_add_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}
#endif
// generation of operation sub backend bitmask

#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(2) Long vr_bitmasksub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_bitmask_sub_double(*arg1, *arg2, &res, backend_bitmask_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmasksub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_bitmask_sub_double(arg1[0], arg2[0], res, backend_bitmask_context);
  interflop_bitmask_sub_double(arg1[1], arg2[1], res + 1, backend_bitmask_context);
}

static VG_REGPARM(3) void vr_bitmasksub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_bitmask_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(2) Int vr_bitmasksub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_bitmask_sub_float(*arg1, *arg2, &res, backend_bitmask_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmasksub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_bitmask_sub_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(3) void vr_bitmasksub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_bitmask_sub_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}
#endif
// generation of operation mul backend bitmask

#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(2) Long vr_bitmaskmul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_bitmask_mul_double(*arg1, *arg2, &res, backend_bitmask_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskmul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_bitmask_mul_double(arg1[0], arg2[0], res, backend_bitmask_context);
  interflop_bitmask_mul_double(arg1[1], arg2[1], res + 1, backend_bitmask_context);
}

static VG_REGPARM(3) void vr_bitmaskmul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_bitmask_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(2) Int vr_bitmaskmul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_bitmask_mul_float(*arg1, *arg2, &res, backend_bitmask_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskmul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_bitmask_mul_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(3) void vr_bitmaskmul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_bitmask_mul_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}
#endif
// generation of operation div backend bitmask

#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(2) Long vr_bitmaskdiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_bitmask_div_double(*arg1, *arg2, &res, backend_bitmask_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskdiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_bitmask_div_double(arg1[0], arg2[0], res, backend_bitmask_context);
  interflop_bitmask_div_double(arg1[1], arg2[1], res + 1, backend_bitmask_context);
}

static VG_REGPARM(3) void vr_bitmaskdiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_bitmask_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(2) Int vr_bitmaskdiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_bitmask_div_float(*arg1, *arg2, &res, backend_bitmask_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskdiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_bitmask_div_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}

static VG_REGPARM(3) void vr_bitmaskdiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_bitmask_div_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
  }
}
#endif
// generation of operation add backend mcaint

#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(2) Long vr_mcaintadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaint_add_double(*arg1, *arg2, &res, backend_mcaint_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaint_add_double(arg1[0], arg2[0], res, backend_mcaint_context);
  interflop_mcaint_add_double(arg1[1], arg2[1], res + 1, backend_mcaint_context);
}

static VG_REGPARM(3) void vr_mcaintadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaint_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(2) Int vr_mcaintadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaint_add_float(*arg1, *arg2, &res, backend_mcaint_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaint_add_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(3) void vr_mcaintadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaint_add_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}
#endif
// generation of operation sub backend mcaint

#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(2) Long vr_mcaintsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaint_sub_double(*arg1, *arg2, &res, backend_mcaint_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaint_sub_double(arg1[0], arg2[0], res, backend_mcaint_context);
  interflop_mcaint_sub_double(arg1[1], arg2[1], res + 1, backend_mcaint_context);
}

static VG_REGPARM(3) void vr_mcaintsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaint_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(2) Int vr_mcaintsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaint_sub_float(*arg1, *arg2, &res, backend_mcaint_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaint_sub_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(3) void vr_mcaintsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaint_sub_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}
#endif
// generation of operation mul backend mcaint

#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(2) Long vr_mcaintmul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaint_mul_double(*arg1, *arg2, &res, backend_mcaint_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintmul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaint_mul_double(arg1[0], arg2[0], res, backend_mcaint_context);
  interflop_mcaint_mul_double(arg1[1], arg2[1], res + 1, backend_mcaint_context);
}

static VG_REGPARM(3) void vr_mcaintmul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaint_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(2) Int vr_mcaintmul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaint_mul_float(*arg1, *arg2, &res, backend_mcaint_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintmul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaint_mul_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(3) void vr_mcaintmul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaint_mul_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}
#endif
// generation of operation div backend mcaint

#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(2) Long vr_mcaintdiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaint_div_double(*arg1, *arg2, &res, backend_mcaint_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintdiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaint_div_double(arg1[0], arg2[0], res, backend_mcaint_context);
  interflop_mcaint_div_double(arg1[1], arg2[1], res + 1, backend_mcaint_context);
}

static VG_REGPARM(3) void vr_mcaintdiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaint_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(2) Int vr_mcaintdiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaint_div_float(*arg1, *arg2, &res, backend_mcaint_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintdiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaint_div_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}

static VG_REGPARM(3) void vr_mcaintdiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaint_div_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
  }
}
#endif
// generation of operation add backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroucheckfloatmaxadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_add_double(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkfloatmax_add_double(arg1[0], arg2[0], res, backend_checkfloatmax_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
  interflop_checkfloatmax_add_double(arg1[1], arg2[1], res + 1, backend_checkfloatmax_context);
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckfloatmaxadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_add_float(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_add_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_add_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}
#endif
// generation of operation sub backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroucheckfloatmaxsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_sub_double(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkfloatmax_sub_double(arg1[0], arg2[0], res, backend_checkfloatmax_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
  interflop_checkfloatmax_sub_double(arg1[1], arg2[1], res + 1, backend_checkfloatmax_context);
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckfloatmaxsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_sub_float(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_sub_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_sub_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}
#endif
// generation of operation mul backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroucheckfloatmaxmul64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_mul_double(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxmul64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkfloatmax_mul_double(arg1[0], arg2[0], res, backend_checkfloatmax_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
  interflop_checkfloatmax_mul_double(arg1[1], arg2[1], res + 1, backend_checkfloatmax_context);
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxmul64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckfloatmaxmul32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_mul_float(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxmul32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_mul_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxmul32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_mul_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}
#endif
// generation of operation div backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroucheckfloatmaxdiv64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_div_double(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxdiv64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkfloatmax_div_double(arg1[0], arg2[0], res, backend_checkfloatmax_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
  interflop_checkfloatmax_div_double(arg1[1], arg2[1], res + 1, backend_checkfloatmax_context);
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxdiv64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_div_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckfloatmaxdiv32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkfloatmax_div_float(*arg1, *arg2, &res, backend_checkfloatmax_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxdiv32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_div_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckfloatmaxdiv32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkfloatmax_div_float(arg1[i], arg2[i], res + i, backend_checkfloatmax_context);
  }
}
#endif
// generation of operation add backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroucheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation sub backend verrou

#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(2) Long vr_verroucheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res + 1, backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i, backend_verrou_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation add backend mcaquad

#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(2) Long vr_mcaquadcheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res + 1, backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaquad_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation sub backend mcaquad

#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(2) Long vr_mcaquadcheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res + 1, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaquad_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i, backend_mcaquad_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation add backend checkdenormal

#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(2) Long vr_checkdenormalcheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenormal_add_double(*arg1, *arg2, &res, backend_checkdenormal_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenormal_add_double(arg1[0], arg2[0], res, backend_checkdenormal_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_checkdenormal_add_double(arg1[1], arg2[1], res + 1, backend_checkdenormal_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkdenormal_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormalcheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenormal_add_float(*arg1, *arg2, &res, backend_checkdenormal_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenormal_add_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_add_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation sub backend checkdenormal

#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(2) Long vr_checkdenormalcheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenormal_sub_double(*arg1, *arg2, &res, backend_checkdenormal_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenormal_sub_double(arg1[0], arg2[0], res, backend_checkdenormal_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_checkdenormal_sub_double(arg1[1], arg2[1], res + 1, backend_checkdenormal_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkdenormal_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormalcheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenormal_sub_float(*arg1, *arg2, &res, backend_checkdenormal_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenormal_sub_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormalcheckcancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenormal_sub_float(arg1[i], arg2[i], res + i, backend_checkdenormal_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation add backend vprec

#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(2) Long vr_vpreccheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_add_double(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_add_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_vprec_add_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_vpreccheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_add_float(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation sub backend vprec

#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(2) Long vr_vpreccheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_sub_double(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_sub_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_vprec_sub_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_vpreccheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_sub_float(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation add backend cancellation

#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(2) Long vr_cancellationcheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_cancellation_add_double(*arg1, *arg2, &res, backend_cancellation_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_cancellation_add_double(arg1[0], arg2[0], res, backend_cancellation_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_cancellation_add_double(arg1[1], arg2[1], res + 1, backend_cancellation_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_cancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_cancellation_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_cancellationcheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_cancellation_add_float(*arg1, *arg2, &res, backend_cancellation_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_cancellation_add_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_cancellation_add_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation sub backend cancellation

#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(2) Long vr_cancellationcheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_cancellation_sub_double(*arg1, *arg2, &res, backend_cancellation_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_cancellation_sub_double(arg1[0], arg2[0], res, backend_cancellation_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_cancellation_sub_double(arg1[1], arg2[1], res + 1, backend_cancellation_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_cancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_cancellation_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_cancellationcheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_cancellation_sub_float(*arg1, *arg2, &res, backend_cancellation_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_cancellation_sub_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_cancellationcheckcancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_cancellation_sub_float(arg1[i], arg2[i], res + i, backend_cancellation_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation add backend bitmask

#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(2) Long vr_bitmaskcheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_bitmask_add_double(*arg1, *arg2, &res, backend_bitmask_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_bitmask_add_double(arg1[0], arg2[0], res, backend_bitmask_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_bitmask_add_double(arg1[1], arg2[1], res + 1, backend_bitmask_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_bitmask_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_bitmask_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_bitmaskcheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_bitmask_add_float(*arg1, *arg2, &res, backend_bitmask_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_bitmask_add_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_bitmask_add_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation sub backend bitmask

#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(2) Long vr_bitmaskcheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_bitmask_sub_double(*arg1, *arg2, &res, backend_bitmask_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_bitmask_sub_double(arg1[0], arg2[0], res, backend_bitmask_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_bitmask_sub_double(arg1[1], arg2[1], res + 1, backend_bitmask_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_bitmask_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_bitmask_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_bitmaskcheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_bitmask_sub_float(*arg1, *arg2, &res, backend_bitmask_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_bitmask_sub_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_bitmaskcheckcancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_bitmask_sub_float(arg1[i], arg2[i], res + i, backend_bitmask_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation add backend mcaint

#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(2) Long vr_mcaintcheckcancellationadd64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaint_add_double(*arg1, *arg2, &res, backend_mcaint_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationadd64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaint_add_double(arg1[0], arg2[0], res, backend_mcaint_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaint_add_double(arg1[1], arg2[1], res + 1, backend_mcaint_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationadd64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaint_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaint_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaintcheckcancellationadd32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaint_add_float(*arg1, *arg2, &res, backend_mcaint_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationadd32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaint_add_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationadd32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaint_add_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation sub backend mcaint

#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(2) Long vr_mcaintcheckcancellationsub64F (Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaint_sub_double(*arg1, *arg2, &res, backend_mcaint_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res, backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationsub64Fx2 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaint_sub_double(arg1[0], arg2[0], res, backend_mcaint_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res, backend_checkcancellation_context);
  interflop_mcaint_sub_double(arg1[1], arg2[1], res + 1, backend_mcaint_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1, backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationsub64Fx4 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaint_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_mcaint_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaintcheckcancellationsub32F (Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaint_sub_float(*arg1, *arg2, &res, backend_mcaint_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res, backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationsub32Fx8 (/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaint_sub_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaintcheckcancellationsub32Fx4 (/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaint_sub_float(arg1[i], arg2[i], res + i, backend_mcaint_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i, backend_checkcancellation_context);
  }
}
#endif
// generation of operation madd backend verrou

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Long vr_verroumadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_fma_double(*arg1, *arg2,  * arg3, &res, backend_verrou_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_fma_float(*arg1, *arg2,  * arg3, &res, backend_verrou_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend verrou

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Long vr_verroumsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_fma_double(*arg1, *arg2, - * arg3, &res, backend_verrou_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_fma_float(*arg1, *arg2, - * arg3, &res, backend_verrou_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend mcaquad

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(3) Long vr_mcaquadmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_fma_double(*arg1, *arg2,  * arg3, &res, backend_mcaquad_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_fma_float(*arg1, *arg2,  * arg3, &res, backend_mcaquad_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend mcaquad

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(3) Long vr_mcaquadmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_fma_double(*arg1, *arg2, - * arg3, &res, backend_mcaquad_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_fma_float(*arg1, *arg2, - * arg3, &res, backend_mcaquad_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend checkdenormal

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(3) Long vr_checkdenormalmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenormal_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkdenormal_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormalmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenormal_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkdenormal_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend checkdenormal

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(3) Long vr_checkdenormalmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenormal_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkdenormal_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormalmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenormal_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkdenormal_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend vprec

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(3) Long vr_vprecmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_fma_double(*arg1, *arg2,  * arg3, &res, backend_vprec_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_vprecmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_fma_float(*arg1, *arg2,  * arg3, &res, backend_vprec_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend vprec

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(3) Long vr_vprecmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_fma_double(*arg1, *arg2, - * arg3, &res, backend_vprec_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_vprecmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_fma_float(*arg1, *arg2, - * arg3, &res, backend_vprec_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend cancellation

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(3) Long vr_cancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_cancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_cancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_cancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_cancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_cancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend cancellation

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(3) Long vr_cancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_cancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_cancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_cancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_cancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_cancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend bitmask

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(3) Long vr_bitmaskmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_bitmask_fma_double(*arg1, *arg2,  * arg3, &res, backend_bitmask_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_bitmaskmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_bitmask_fma_float(*arg1, *arg2,  * arg3, &res, backend_bitmask_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend bitmask

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(3) Long vr_bitmaskmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_bitmask_fma_double(*arg1, *arg2, - * arg3, &res, backend_bitmask_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_bitmaskmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_bitmask_fma_float(*arg1, *arg2, - * arg3, &res, backend_bitmask_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend mcaint

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(3) Long vr_mcaintmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaint_fma_double(*arg1, *arg2,  * arg3, &res, backend_mcaint_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaintmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaint_fma_float(*arg1, *arg2,  * arg3, &res, backend_mcaint_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend mcaint

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(3) Long vr_mcaintmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaint_fma_double(*arg1, *arg2, - * arg3, &res, backend_mcaint_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaintmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaint_fma_float(*arg1, *arg2, - * arg3, &res, backend_mcaint_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend verrou

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Long vr_verroucheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_fma_double(*arg1, *arg2,  * arg3, &res, backend_verrou_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_fma_float(*arg1, *arg2,  * arg3, &res, backend_verrou_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend verrou

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Long vr_verroucheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_fma_double(*arg1, *arg2, - * arg3, &res, backend_verrou_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_fma_float(*arg1, *arg2, - * arg3, &res, backend_verrou_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend mcaquad

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(3) Long vr_mcaquadcheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_fma_double(*arg1, *arg2,  * arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadcheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_fma_float(*arg1, *arg2,  * arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend mcaquad

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAQUAD
static VG_REGPARM(3) Long vr_mcaquadcheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_fma_double(*arg1, *arg2, - * arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadcheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_fma_float(*arg1, *arg2, - * arg3, &res, backend_mcaquad_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend checkdenormal

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(3) Long vr_checkdenormalcheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenormal_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkdenormal_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormalcheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenormal_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkdenormal_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend checkdenormal

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CHECKDENORMAL
static VG_REGPARM(3) Long vr_checkdenormalcheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenormal_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkdenormal_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormalcheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenormal_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkdenormal_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend vprec

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(3) Long vr_vpreccheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_fma_double(*arg1, *arg2,  * arg3, &res, backend_vprec_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_vpreccheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_fma_float(*arg1, *arg2,  * arg3, &res, backend_vprec_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend vprec

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VPREC
static VG_REGPARM(3) Long vr_vpreccheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_fma_double(*arg1, *arg2, - * arg3, &res, backend_vprec_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_vpreccheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_fma_float(*arg1, *arg2, - * arg3, &res, backend_vprec_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend cancellation

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(3) Long vr_cancellationcheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_cancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_cancellation_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_cancellationcheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_cancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_cancellation_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend cancellation

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_CANCELLATION
static VG_REGPARM(3) Long vr_cancellationcheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_cancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_cancellation_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_cancellationcheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_cancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_cancellation_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend bitmask

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(3) Long vr_bitmaskcheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_bitmask_fma_double(*arg1, *arg2,  * arg3, &res, backend_bitmask_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_bitmaskcheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_bitmask_fma_float(*arg1, *arg2,  * arg3, &res, backend_bitmask_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend bitmask

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_BITMASK
static VG_REGPARM(3) Long vr_bitmaskcheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_bitmask_fma_double(*arg1, *arg2, - * arg3, &res, backend_bitmask_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_bitmaskcheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_bitmask_fma_float(*arg1, *arg2, - * arg3, &res, backend_bitmask_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend mcaint

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(3) Long vr_mcaintcheckcancellationmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaint_fma_double(*arg1, *arg2,  * arg3, &res, backend_mcaint_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaintcheckcancellationmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaint_fma_float(*arg1, *arg2,  * arg3, &res, backend_mcaint_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend mcaint

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_MCAINT
static VG_REGPARM(3) Long vr_mcaintcheckcancellationmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaint_fma_double(*arg1, *arg2, - * arg3, &res, backend_mcaint_context);
  interflop_checkcancellation_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaintcheckcancellationmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaint_fma_float(*arg1, *arg2, - * arg3, &res, backend_mcaint_context);
  interflop_checkcancellation_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation madd backend verrou

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Long vr_verroucheckfloatmaxmadd64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_fma_double(*arg1, *arg2,  * arg3, &res, backend_verrou_context);
  interflop_checkfloatmax_fma_double(*arg1, *arg2,  * arg3, &res, backend_checkfloatmax_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckfloatmaxmadd32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_fma_float(*arg1, *arg2,  * arg3, &res, backend_verrou_context);
  interflop_checkfloatmax_fma_float(*arg1, *arg2,  * arg3, &res, backend_checkfloatmax_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif// generation of operation msub backend verrou

// FMA Operator
#ifdef LINK_INTERFLOP_BACKEND_VERROU
static VG_REGPARM(3) Long vr_verroucheckfloatmaxmsub64F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_fma_double(*arg1, *arg2, - * arg3, &res, backend_verrou_context);
  interflop_checkfloatmax_fma_double(*arg1, *arg2, - * arg3, &res, backend_checkfloatmax_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroucheckfloatmaxmsub32F (Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_fma_float(*arg1, *arg2, - * arg3, &res, backend_verrou_context);
  interflop_checkfloatmax_fma_float(*arg1, *arg2, - * arg3, &res, backend_checkfloatmax_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
#endif