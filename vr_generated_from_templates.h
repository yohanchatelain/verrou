// Generated by './generateBackendInterOperator.py'
//  generation of operation cast backend verrou

static VG_REGPARM(3) Int vr_verroucast64FTo32F(Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_verrou_cast_double_to_float(*arg1, &res, backend_verrou_context);
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation cast backend mcaquad

static VG_REGPARM(3) Int vr_mcaquadcast64FTo32F(Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_mcaquad_cast_double_to_float(*arg1, &res, backend_mcaquad_context);
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation cast backend checkdenorm

static VG_REGPARM(3) Int vr_checkdenormcast64FTo32F(Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_checkdenorm_cast_double_to_float(*arg1, &res,
                                             backend_checkdenorm_context);
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation cast backend vprec

static VG_REGPARM(3) Int vr_vpreccast64FTo32F(Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_vprec_cast_double_to_float(*arg1, &res, backend_vprec_context);
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation cast backend verrou

static VG_REGPARM(3) Int vr_verroucheck_float_maxcast64FTo32F(Long a) {
  double *arg1 = (double *)(&a);
  float res;
  interflop_verrou_cast_double_to_float(*arg1, &res, backend_verrou_context);
  interflop_check_float_max_cast_double_to_float(
      *arg1, &res, backend_check_float_max_context);
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation add backend verrou

static VG_REGPARM(2) Long vr_verrouadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrouadd64Fx4(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrouadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrouadd32Fx8(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrouadd32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

// generation of operation sub backend verrou

static VG_REGPARM(2) Long vr_verrousub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
}

static VG_REGPARM(3) void vr_verrousub64Fx4(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verrousub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verrousub32Fx8(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verrousub32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

// generation of operation mul backend verrou

static VG_REGPARM(2) Long vr_verroumul64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroumul64Fx4(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroumul32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroumul32Fx8(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroumul32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

// generation of operation div backend verrou

static VG_REGPARM(2) Long vr_verroudiv64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
}

static VG_REGPARM(3) void vr_verroudiv64Fx4(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
  }
}

static VG_REGPARM(2) Int vr_verroudiv32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroudiv32Fx8(/*OUT*/ V256 *output, ULong b0,
                                            ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

static VG_REGPARM(3) void vr_verroudiv32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                            ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
  }
}

// generation of operation add backend mcaquad

static VG_REGPARM(2) Long vr_mcaquadadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res + 1,
                               backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadadd64Fx4(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                 backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx8(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadadd32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

// generation of operation sub backend mcaquad

static VG_REGPARM(2) Long vr_mcaquadsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res + 1,
                               backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadsub64Fx4(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                 backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx8(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadsub32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

// generation of operation mul backend mcaquad

static VG_REGPARM(2) Long vr_mcaquadmul64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_mul_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_mul_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_mul_double(arg1[1], arg2[1], res + 1,
                               backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquadmul64Fx4(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                 backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadmul32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_mul_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx8(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_mul_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadmul32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_mul_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

// generation of operation div backend mcaquad

static VG_REGPARM(2) Long vr_mcaquaddiv64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_div_double(*arg1, *arg2, &res, backend_mcaquad_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_div_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_mcaquad_div_double(arg1[1], arg2[1], res + 1,
                               backend_mcaquad_context);
}

static VG_REGPARM(3) void vr_mcaquaddiv64Fx4(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_div_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                 backend_mcaquad_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquaddiv32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_div_float(*arg1, *arg2, &res, backend_mcaquad_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx8(/*OUT*/ V256 *output, ULong b0,
                                             ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_div_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

static VG_REGPARM(3) void vr_mcaquaddiv32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                             ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_div_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
  }
}

// generation of operation add backend checkdenorm

static VG_REGPARM(2) Long vr_checkdenormadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenorm_add_double(*arg1, *arg2, &res,
                                   backend_checkdenorm_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormadd64Fx2(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenorm_add_double(arg1[0], arg2[0], res,
                                   backend_checkdenorm_context);
  interflop_checkdenorm_add_double(arg1[1], arg2[1], res + 1,
                                   backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormadd64Fx4(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                     backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenorm_add_float(*arg1, *arg2, &res,
                                  backend_checkdenorm_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormadd32Fx8(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenorm_add_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormadd32Fx4(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_add_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

// generation of operation sub backend checkdenorm

static VG_REGPARM(2) Long vr_checkdenormsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenorm_sub_double(*arg1, *arg2, &res,
                                   backend_checkdenorm_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormsub64Fx2(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenorm_sub_double(arg1[0], arg2[0], res,
                                   backend_checkdenorm_context);
  interflop_checkdenorm_sub_double(arg1[1], arg2[1], res + 1,
                                   backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormsub64Fx4(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                     backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenorm_sub_float(*arg1, *arg2, &res,
                                  backend_checkdenorm_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormsub32Fx8(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenorm_sub_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormsub32Fx4(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_sub_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

// generation of operation mul backend checkdenorm

static VG_REGPARM(2) Long vr_checkdenormmul64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenorm_mul_double(*arg1, *arg2, &res,
                                   backend_checkdenorm_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormmul64Fx2(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenorm_mul_double(arg1[0], arg2[0], res,
                                   backend_checkdenorm_context);
  interflop_checkdenorm_mul_double(arg1[1], arg2[1], res + 1,
                                   backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormmul64Fx4(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                     backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormmul32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenorm_mul_float(*arg1, *arg2, &res,
                                  backend_checkdenorm_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormmul32Fx8(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenorm_mul_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormmul32Fx4(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_mul_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

// generation of operation div backend checkdenorm

static VG_REGPARM(2) Long vr_checkdenormdiv64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenorm_div_double(*arg1, *arg2, &res,
                                   backend_checkdenorm_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormdiv64Fx2(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenorm_div_double(arg1[0], arg2[0], res,
                                   backend_checkdenorm_context);
  interflop_checkdenorm_div_double(arg1[1], arg2[1], res + 1,
                                   backend_checkdenorm_context);
}

static VG_REGPARM(3) void vr_checkdenormdiv64Fx4(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_div_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                     backend_checkdenorm_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormdiv32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenorm_div_float(*arg1, *arg2, &res,
                                  backend_checkdenorm_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormdiv32Fx8(/*OUT*/ V256 *output, ULong b0,
                                                 ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenorm_div_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormdiv32Fx4(/*OUT*/ V128 *output,
                                                 ULong aHi, ULong aLo,
                                                 ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_div_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
  }
}

// generation of operation add backend vprec

static VG_REGPARM(2) Long vr_vprecadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_add_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecadd64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_add_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_add_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecadd64Fx4(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                               backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_add_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecadd32Fx8(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecadd32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

// generation of operation sub backend vprec

static VG_REGPARM(2) Long vr_vprecsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_sub_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecsub64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_sub_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_sub_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecsub64Fx4(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                               backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_sub_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecsub32Fx8(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecsub32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

// generation of operation mul backend vprec

static VG_REGPARM(2) Long vr_vprecmul64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_mul_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecmul64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_mul_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_mul_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecmul64Fx4(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                               backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecmul32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_mul_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecmul32Fx8(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_mul_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecmul32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_mul_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

// generation of operation div backend vprec

static VG_REGPARM(2) Long vr_vprecdiv64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_div_double(*arg1, *arg2, &res, backend_vprec_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecdiv64Fx2(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_div_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_vprec_div_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
}

static VG_REGPARM(3) void vr_vprecdiv64Fx4(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_div_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                               backend_vprec_context);
  }
}

static VG_REGPARM(2) Int vr_vprecdiv32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_div_float(*arg1, *arg2, &res, backend_vprec_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vprecdiv32Fx8(/*OUT*/ V256 *output, ULong b0,
                                           ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_div_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

static VG_REGPARM(3) void vr_vprecdiv32Fx4(/*OUT*/ V128 *output, ULong aHi,
                                           ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_div_float(arg1[i], arg2[i], res + i, backend_vprec_context);
  }
}

// generation of operation add backend verrou

static VG_REGPARM(2) Long vr_verroucheck_float_maxadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_add_double(*arg1, *arg2, &res,
                                       backend_check_float_max_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd64Fx2(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_add_double(arg1[0], arg2[0], res,
                                       backend_check_float_max_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
  interflop_check_float_max_add_double(arg1[1], arg2[1], res + 1,
                                       backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd64Fx4(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
    interflop_check_float_max_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                         backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_add_float(*arg1, *arg2, &res,
                                      backend_check_float_max_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd32Fx8(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_add_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxadd32Fx4(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_add_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

// generation of operation sub backend verrou

static VG_REGPARM(2) Long vr_verroucheck_float_maxsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_sub_double(*arg1, *arg2, &res,
                                       backend_check_float_max_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub64Fx2(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_sub_double(arg1[0], arg2[0], res,
                                       backend_check_float_max_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
  interflop_check_float_max_sub_double(arg1[1], arg2[1], res + 1,
                                       backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub64Fx4(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
    interflop_check_float_max_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                         backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_sub_float(*arg1, *arg2, &res,
                                      backend_check_float_max_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub32Fx8(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_sub_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxsub32Fx4(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_sub_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

// generation of operation mul backend verrou

static VG_REGPARM(2) Long vr_verroucheck_float_maxmul64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_mul_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_mul_double(*arg1, *arg2, &res,
                                       backend_check_float_max_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul64Fx2(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_mul_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_mul_double(arg1[0], arg2[0], res,
                                       backend_check_float_max_context);
  interflop_verrou_mul_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
  interflop_check_float_max_mul_double(arg1[1], arg2[1], res + 1,
                                       backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul64Fx4(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
    interflop_check_float_max_mul_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                         backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxmul32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_mul_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_mul_float(*arg1, *arg2, &res,
                                      backend_check_float_max_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul32Fx8(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_mul_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxmul32Fx4(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_mul_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_mul_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

// generation of operation div backend verrou

static VG_REGPARM(2) Long vr_verroucheck_float_maxdiv64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_div_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_div_double(*arg1, *arg2, &res,
                                       backend_check_float_max_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv64Fx2(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_div_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_check_float_max_div_double(arg1[0], arg2[0], res,
                                       backend_check_float_max_context);
  interflop_verrou_div_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
  interflop_check_float_max_div_double(arg1[1], arg2[1], res + 1,
                                       backend_check_float_max_context);
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv64Fx4(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
    interflop_check_float_max_div_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                         backend_check_float_max_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheck_float_maxdiv32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_div_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_check_float_max_div_float(*arg1, *arg2, &res,
                                      backend_check_float_max_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv32Fx8(/*OUT*/ V256 *output,
                                                           ULong b0, ULong b1,
                                                           ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_div_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

static VG_REGPARM(3) void vr_verroucheck_float_maxdiv32Fx4(/*OUT*/ V128 *output,
                                                           ULong aHi, ULong aLo,
                                                           ULong bHi,
                                                           ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_div_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_check_float_max_div_float(arg1[i], arg2[i], res + i,
                                        backend_check_float_max_context);
  }
}

// generation of operation add backend verrou

static VG_REGPARM(2) Long vr_verroucheckcancellationadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_add_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_add_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_verrou_add_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckcancellationadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_add_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckcancellationadd32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_add_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation sub backend verrou

static VG_REGPARM(2) Long vr_verroucheckcancellationsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_verrou_sub_double(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_verrou_sub_double(arg1[0], arg2[0], res, backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_verrou_sub_double(arg1[1], arg2[1], res + 1,
                              backend_verrou_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                backend_verrou_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_verroucheckcancellationsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_verrou_sub_float(*arg1, *arg2, &res, backend_verrou_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_verroucheckcancellationsub32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_verrou_sub_float(arg1[i], arg2[i], res + i,
                               backend_verrou_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation add backend mcaquad

static VG_REGPARM(2) Long vr_mcaquadcheckcancellationadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_add_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_add_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_mcaquad_add_double(arg1[1], arg2[1], res + 1,
                               backend_mcaquad_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                 backend_mcaquad_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellationadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_add_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationadd32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_add_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation sub backend mcaquad

static VG_REGPARM(2) Long vr_mcaquadcheckcancellationsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_mcaquad_sub_double(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_mcaquad_sub_double(arg1[0], arg2[0], res, backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_mcaquad_sub_double(arg1[1], arg2[1], res + 1,
                               backend_mcaquad_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                 backend_mcaquad_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_mcaquadcheckcancellationsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_mcaquad_sub_float(*arg1, *arg2, &res, backend_mcaquad_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_mcaquadcheckcancellationsub32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_mcaquad_sub_float(arg1[i], arg2[i], res + i,
                                backend_mcaquad_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation add backend checkdenorm

static VG_REGPARM(2) Long
    vr_checkdenormcheckcancellationadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenorm_add_double(*arg1, *arg2, &res,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenorm_add_double(arg1[0], arg2[0], res,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_checkdenorm_add_double(arg1[1], arg2[1], res + 1,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                     backend_checkdenorm_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormcheckcancellationadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenorm_add_float(*arg1, *arg2, &res,
                                  backend_checkdenorm_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenorm_add_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationadd32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_add_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation sub backend checkdenorm

static VG_REGPARM(2) Long
    vr_checkdenormcheckcancellationsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_checkdenorm_sub_double(*arg1, *arg2, &res,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_checkdenorm_sub_double(arg1[0], arg2[0], res,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_checkdenorm_sub_double(arg1[1], arg2[1], res + 1,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                                     backend_checkdenorm_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_checkdenormcheckcancellationsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_checkdenorm_sub_float(*arg1, *arg2, &res,
                                  backend_checkdenorm_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_checkdenorm_sub_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_checkdenormcheckcancellationsub32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_checkdenorm_sub_float(arg1[i], arg2[i], res + i,
                                    backend_checkdenorm_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation add backend vprec

static VG_REGPARM(2) Long vr_vpreccheckcancellationadd64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_add_double(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_add_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_add_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_checkcancellation_add_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_vprec_add_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
  interflop_checkcancellation_add_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                               backend_vprec_context);
    interflop_checkcancellation_add_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_vpreccheckcancellationadd32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_add_float(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_add_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_vpreccheckcancellationadd32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_add_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_add_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation sub backend vprec

static VG_REGPARM(2) Long vr_vpreccheckcancellationsub64F(Long a, Long b) {
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double res;
  interflop_vprec_sub_double(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_sub_double(*arg1, *arg2, &res,
                                         backend_checkcancellation_context);
  Long *c = (Long *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub64Fx2(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  double arg1[2] = {*((double *)(&aLo)), *((double *)(&aHi))};
  double arg2[2] = {*((double *)(&bLo)), *((double *)(&bHi))};
  double *res = (double *)output;
  interflop_vprec_sub_double(arg1[0], arg2[0], res, backend_vprec_context);
  interflop_checkcancellation_sub_double(arg1[0], arg2[0], res,
                                         backend_checkcancellation_context);
  interflop_vprec_sub_double(arg1[1], arg2[1], res + 1, backend_vprec_context);
  interflop_checkcancellation_sub_double(arg1[1], arg2[1], res + 1,
                                         backend_checkcancellation_context);
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub64Fx4(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {

  double arg2[4] = {*((double *)(&b0)), *((double *)(&b1)), *((double *)(&b2)),
                    *((double *)(&b3))};
  double *res = (double *)output;
  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_double(arg1CopyAvxDouble[i], arg2[i], res + i,
                               backend_vprec_context);
    interflop_checkcancellation_sub_double(arg1CopyAvxDouble[i], arg2[i],
                                           res + i,
                                           backend_checkcancellation_context);
  }
}

static VG_REGPARM(2) Int vr_vpreccheckcancellationsub32F(Long a, Long b) {
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float res;
  interflop_vprec_sub_float(*arg1, *arg2, &res, backend_vprec_context);
  interflop_checkcancellation_sub_float(*arg1, *arg2, &res,
                                        backend_checkcancellation_context);
  Int *c = (Int *)(&res);
  return *c;
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub32Fx8(
    /*OUT*/ V256 *output, ULong b0, ULong b1, ULong b2, ULong b3) {
  V256 reg2;
  reg2.w64[0] = b0;
  reg2.w64[1] = b1;
  reg2.w64[2] = b2;
  reg2.w64[3] = b3;
  float *res = (float *)output;
  float *arg1 = arg1CopyAvxFloat;
  float *arg2 = (float *)&reg2;
  for (int i = 0; i < 8; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

static VG_REGPARM(3) void vr_vpreccheckcancellationsub32Fx4(
    /*OUT*/ V128 *output, ULong aHi, ULong aLo, ULong bHi, ULong bLo) {
  V128 reg1;
  reg1.w64[0] = aLo;
  reg1.w64[1] = aHi;
  V128 reg2;
  reg2.w64[0] = bLo;
  reg2.w64[1] = bHi;

  float *res = (float *)output;
  float *arg1 = (float *)&reg1;
  float *arg2 = (float *)&reg2;

  for (int i = 0; i < 4; i++) {
    interflop_vprec_sub_float(arg1[i], arg2[i], res + i, backend_vprec_context);
    interflop_checkcancellation_sub_float(arg1[i], arg2[i], res + i,
                                          backend_checkcancellation_context);
  }
}

// generation of operation madd backend verrou
// FMA Operator
static VG_REGPARM(3) Long vr_verroumadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, *arg3, &res,
                               backend_verrou_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, *arg3, &res,
                              backend_verrou_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend verrou
// FMA Operator
static VG_REGPARM(3) Long vr_verroumsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, -*arg3, &res,
                               backend_verrou_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_verroumsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, -*arg3, &res,
                              backend_verrou_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend mcaquad
// FMA Operator
static VG_REGPARM(3) Long vr_mcaquadmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2, *arg3, &res,
                                backend_mcaquad_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2, *arg3, &res,
                               backend_mcaquad_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend mcaquad
// FMA Operator
static VG_REGPARM(3) Long vr_mcaquadmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2, -*arg3, &res,
                                backend_mcaquad_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_mcaquadmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2, -*arg3, &res,
                               backend_mcaquad_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend checkdenorm
// FMA Operator
static VG_REGPARM(3) Long vr_checkdenormmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2, *arg3, &res,
                                    backend_checkdenorm_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2, *arg3, &res,
                                   backend_checkdenorm_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend checkdenorm
// FMA Operator
static VG_REGPARM(3) Long vr_checkdenormmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2, -*arg3, &res,
                                    backend_checkdenorm_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_checkdenormmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2, -*arg3, &res,
                                   backend_checkdenorm_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend vprec
// FMA Operator
static VG_REGPARM(3) Long vr_vprecmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_madd_double(*arg1, *arg2, *arg3, &res, backend_vprec_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_vprecmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_madd_float(*arg1, *arg2, *arg3, &res, backend_vprec_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend vprec
// FMA Operator
static VG_REGPARM(3) Long vr_vprecmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_madd_double(*arg1, *arg2, -*arg3, &res,
                              backend_vprec_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int vr_vprecmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_madd_float(*arg1, *arg2, -*arg3, &res, backend_vprec_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend verrou
// FMA Operator
static VG_REGPARM(3) Long
    vr_verroucheckcancellationmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, *arg3, &res,
                               backend_verrou_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, *arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_verroucheckcancellationmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, *arg3, &res,
                              backend_verrou_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, *arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend verrou
// FMA Operator
static VG_REGPARM(3) Long
    vr_verroucheckcancellationmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, -*arg3, &res,
                               backend_verrou_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, -*arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_verroucheckcancellationmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, -*arg3, &res,
                              backend_verrou_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, -*arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend mcaquad
// FMA Operator
static VG_REGPARM(3) Long
    vr_mcaquadcheckcancellationmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2, *arg3, &res,
                                backend_mcaquad_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, *arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_mcaquadcheckcancellationmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2, *arg3, &res,
                               backend_mcaquad_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, *arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend mcaquad
// FMA Operator
static VG_REGPARM(3) Long
    vr_mcaquadcheckcancellationmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_mcaquad_madd_double(*arg1, *arg2, -*arg3, &res,
                                backend_mcaquad_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, -*arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_mcaquadcheckcancellationmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_mcaquad_madd_float(*arg1, *arg2, -*arg3, &res,
                               backend_mcaquad_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, -*arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend checkdenorm
// FMA Operator
static VG_REGPARM(3) Long
    vr_checkdenormcheckcancellationmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2, *arg3, &res,
                                    backend_checkdenorm_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, *arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_checkdenormcheckcancellationmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2, *arg3, &res,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, *arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend checkdenorm
// FMA Operator
static VG_REGPARM(3) Long
    vr_checkdenormcheckcancellationmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_checkdenorm_madd_double(*arg1, *arg2, -*arg3, &res,
                                    backend_checkdenorm_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, -*arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_checkdenormcheckcancellationmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_checkdenorm_madd_float(*arg1, *arg2, -*arg3, &res,
                                   backend_checkdenorm_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, -*arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend vprec
// FMA Operator
static VG_REGPARM(3) Long
    vr_vpreccheckcancellationmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_madd_double(*arg1, *arg2, *arg3, &res, backend_vprec_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, *arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_vpreccheckcancellationmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_madd_float(*arg1, *arg2, *arg3, &res, backend_vprec_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, *arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend vprec
// FMA Operator
static VG_REGPARM(3) Long
    vr_vpreccheckcancellationmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_vprec_madd_double(*arg1, *arg2, -*arg3, &res,
                              backend_vprec_context);
  interflop_checkcancellation_madd_double(*arg1, *arg2, -*arg3, &res,
                                          backend_checkcancellation_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_vpreccheckcancellationmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_vprec_madd_float(*arg1, *arg2, -*arg3, &res, backend_vprec_context);
  interflop_checkcancellation_madd_float(*arg1, *arg2, -*arg3, &res,
                                         backend_checkcancellation_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation madd backend verrou
// FMA Operator
static VG_REGPARM(3) Long
    vr_verroucheck_float_maxmadd64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, *arg3, &res,
                               backend_verrou_context);
  interflop_check_float_max_madd_double(*arg1, *arg2, *arg3, &res,
                                        backend_check_float_max_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_verroucheck_float_maxmadd32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, *arg3, &res,
                              backend_verrou_context);
  interflop_check_float_max_madd_float(*arg1, *arg2, *arg3, &res,
                                       backend_check_float_max_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
// generation of operation msub backend verrou
// FMA Operator
static VG_REGPARM(3) Long
    vr_verroucheck_float_maxmsub64F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  double *arg1 = (double *)(&a);
  double *arg2 = (double *)(&b);
  double *arg3 = (double *)(&c);
  double res;
  interflop_verrou_madd_double(*arg1, *arg2, -*arg3, &res,
                               backend_verrou_context);
  interflop_check_float_max_madd_double(*arg1, *arg2, -*arg3, &res,
                                        backend_check_float_max_context);
#else
  double res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Long *d = (Long *)(&res);
  return *d;
}

static VG_REGPARM(3) Int
    vr_verroucheck_float_maxmsub32F(Long a, Long b, Long c) {
#ifdef USE_VERROU_FMA
  float *arg1 = (float *)(&a);
  float *arg2 = (float *)(&b);
  float *arg3 = (float *)(&c);
  float res;
  interflop_verrou_madd_float(*arg1, *arg2, -*arg3, &res,
                              backend_verrou_context);
  interflop_check_float_max_madd_float(*arg1, *arg2, -*arg3, &res,
                                       backend_check_float_max_context);
#else
  float res = 0.;
  VG_(tool_panic)("Verrou needs to be compiled with FMA support \n");
#endif
  Int *d = (Int *)(&res);
  return *d;
}
